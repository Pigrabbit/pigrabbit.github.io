{"componentChunkName":"component---node-modules-lekoarts-gatsby-theme-minimal-blog-core-src-templates-post-query-tsx","path":"/deploying-deep-learning-model-with-react-native","result":{"data":{"post":{"__typename":"MdxPost","slug":"/deploying-deep-learning-model-with-react-native","title":"Deploying Deep Learning model with React Native","date":"24.02.2021","tags":[{"name":"Object Detection","slug":"object-detection"},{"name":"Deep Learning","slug":"deep-learning"},{"name":"Tensorflow.js","slug":"tensorflow-js"},{"name":"React Native","slug":"react-native"}],"description":null,"canonicalUrl":null,"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Deploying Deep Learning model with React Native\",\n  \"date\": \"2021-02-24T00:00:00.000Z\",\n  \"tags\": [\"Object Detection\", \"Deep Learning\", \"Tensorflow.js\", \"React Native\"],\n  \"banner\": \"./tensorflow-js.jpg\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"How I deployed 'Helmet Detection' model with React Native application\")), mdx(\"p\", null, \"The problem I wanted to solve was to check whether person wearing helmet with mobile application.\\nIn here, I used Bikes Helmet Dataset and SSD MobileNet v2 320x320 model(provided by Tensorflow Object Detection API).\\nFirst, I trained the model in Python using Google Colab,\\nthen developed an mobile application with react-native which can inference real-time.\"), mdx(\"h2\", null, \"Defining the task\"), mdx(\"p\", null, \"What I wanted to do with this application was to classify the images by if people wearing bike helmet or not.\\nIt could be achieved by several different approaches.\\nFor example, binary classification can be used to classify the image(wearing a helmet or not)\\nwhen we have raw pixel image as input and label of each image.\\nWe could get to the goal with object detection as well.\"), mdx(\"p\", null, \"Since this project has begun with my personal curiosity,\\nI didn't want to spend too much time on gathering and cleaning dataset.\\nI would rather wanted to find prepared dataset which is good to go.\\nSo here, I decided to choose the approach depending on which dataset I could be available of.\"), mdx(\"p\", null, \"After few hours of googling, luckily, I could find the 'Bikes Helmet Dataset' out.\\nIt includes 764 images and annotations in PascalVOC format.\\nAccordingly, I decided to solve this problem by object detection approach.\"), mdx(\"h2\", null, \"Data preparation\"), mdx(\"p\", null, \"The 'Bikes Helmet Dataset' includes 764 images and annotations in PascalVOC format.\\nThe below is how annotation file looks like.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-xml\"\n  }, \"<annotation>\\n    <folder>images</folder>\\n    <filename>BikesHelmets620.png</filename>\\n    <size>\\n        <width>400</width>\\n        <height>255</height>\\n        <depth>3</depth>\\n    </size>\\n    <segmented>0</segmented>\\n    <object>\\n        <name>With Helmet</name>\\n        <pose>Unspecified</pose>\\n        <truncated>0</truncated>\\n        <occluded>0</occluded>\\n        <difficult>0</difficult>\\n        <bndbox>\\n            <xmin>195</xmin>\\n            <ymin>42</ymin>\\n            <xmax>215</xmax>\\n            <ymax>70</ymax>\\n        </bndbox>\\n    </object>\\n    <object>\\n        <!-- ... -->\\n    </object>\\n    <object>\\n        <!-- ... -->\\n    </object>\\n</annotation>\\n\")), mdx(\"p\", null, \"Next thing what we need to do is parsing the annotation xml file and generating csv file with label information.\\n(plus, splitting the training and test set)\\nThe csv file looks like below.\\nWidth and height column means the size of image and xmin, xmax, ymin, ymax columns represents the bounding box of detected object.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"528px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"31.666666666666664%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAGABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAIDBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAdG8JA//xAAYEAACAwAAAAAAAAAAAAAAAAAAAQIRMv/aAAgBAQABBQKO6Qkf/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFhAAAwAAAAAAAAAAAAAAAAAAABAx/9oACAEBAAY/AiL/xAAYEAEAAwEAAAAAAAAAAAAAAAABABEhMf/aAAgBAQABPyEWh5cNsQAwn//aAAwDAQACAAMAAAAQA8//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAZEAEAAwEBAAAAAAAAAAAAAAABABEhMUH/2gAIAQEAAT8QAAKWMzsB5UbABfhP/9k=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"csv-summary\",\n    \"title\": \"csv-summary\",\n    \"src\": \"/static/bc5dd4bc2eba350b6aca4774b4eb41ed/ea23e/csv-summary.jpg\",\n    \"srcSet\": [\"/static/bc5dd4bc2eba350b6aca4774b4eb41ed/46946/csv-summary.jpg 240w\", \"/static/bc5dd4bc2eba350b6aca4774b4eb41ed/55489/csv-summary.jpg 480w\", \"/static/bc5dd4bc2eba350b6aca4774b4eb41ed/ea23e/csv-summary.jpg 528w\"],\n    \"sizes\": \"(max-width: 528px) 100vw, 528px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \")), mdx(\"h2\", null, \"Training and Validating\"), mdx(\"h4\", null, \"Choosing the model\"), mdx(\"p\", null, \"When it comes to training the model, I followed steps described in \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://blog.tensorflow.org/2021/01/custom-object-detection-in-browser.html\"\n  }, \"Tensorflow Blog\"), \".\\nI wanted to save my time on implementing object detection model on my own.\\n(And the performance of those models already have been proved!)\"), mdx(\"p\", null, \"The reason I choose \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"SSD MobileNet v2 320x320\"), \" model is quite similar as the blog post.\\nI don't need high accuracy but fast speed and small size in order to run it on mobile application.\\nAmong the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\"\n  }, \"TF2 model zoo\"), \",\\nI found that \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"SSD MobileNet v2\"), \" meets my requirements.\"), mdx(\"h4\", null, \"Importing the dataset\"), mdx(\"p\", null, \"Bikes Helmet dataset can be accessed with kaggle cli.\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.kaggle.com/andrewmvd/helmet-detection\"\n  }, \"Here\"), \" is the link to the dataset.\\nIt is much faster to download the dataset with kaggle-cli than upload the dataset in local using google colab filesystem.\"), mdx(\"p\", null, \"Labelmap file is used for defining the classes that are going to be used.\\nAnd the labelmap for our model looks like below.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-pbtxt\"\n  }, \"item {\\n  name: \\\"With Helmet\\\"\\n  id: 1\\n}\\n\\nitem {\\n  name: 'Without Helmet'\\n  id: 2\\n}\\n\")), mdx(\"h4\", null, \"Configuring the training\"), mdx(\"p\", null, \"To speed up the training process, i used the technique so called transfer learning.\\nFirst download the weights of pre-trained model and use it for initializing.\"), mdx(\"p\", null, \"Before configuring the training, reading MobileNetV2 \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.04381\"\n  }, \"paper\"), \" and watching \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.youtube.com/watch?v=7UoOFKcyIvM&t=1415s&ab_channel=JinWonLee\"\n  }, \"recap youtube clip\"), \"\\nhelped me to get the concept of the model.\\nI followed most of hyperparameters setting on blog post excepts the number of classes and the number of steps.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"num_classes = 2\\nbatch_size = 96\\nnum_steps = 5000\\nnum_eval_steps = 1000\\n\")), mdx(\"p\", null, \"It took around 3 hours to get training done(in Colab GPU runtime environment).\\nIt could take longer or shorter depending on the number of classes, number of steps or other hyperparameters.\\nAccording to classification loss graph provided by tensorboard, the classification loss has converged as training steps goes on.\\nThis loss indicates whether bounding box class matches with the predicted class.\"), mdx(\"h4\", null, \"Validation\"), mdx(\"p\", null, \"Following is the result of validation of the trained model using test set.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319\\nAverage Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.670\\nAverage Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.240\\nAverage Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032\\nAverage Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.419\\nAverage Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.303\\nAverage Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.274\\nAverage Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.440\\nAverage Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\\nAverage Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.189\\nAverage Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.559\\nAverage Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.433\\n\\nLoss/localization_loss: 0.261929 INFO:tensorflow:   + Loss/classification_loss: 0.380425\\nLoss/classification_loss: 0.380425 INFO:tensorflow: + Loss/regularization_loss: 0.359017\\nLoss/regularization_loss: 0.359017 INFO:tensorflow: + Loss/total_loss: 1.001371\\nLoss/total_loss: 1.001371\\n\")), mdx(\"p\", null, \"Basically, there are two metrics which indicates performance of the model.\"), mdx(\"p\", null, \"The recall(a.k.a sensitivity, true positive rate) is defines as ratio of true positive to total number of ground truth positive,\\nwhile the precision is defined as ratio of true positive to total number of predicted positive.\"), mdx(\"p\", null, \"In this task, each case could be described as below.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"True Positive: A person is wearing a helmet, model detects a person with a helmet\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"True Negative: A person is not wearing a helmet, model detects a person without a helmet\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"False Positive: A person is not wearing a helmet, model detects a person with a helmet\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"False Negative: A person is wearing a helmet, model detects a person without a helmet\")), mdx(\"p\", null, \"IoU, which stands for Intersection of Union is given by the ratio of the area of intersection and area of union of the predicted bounding box and ground truth bounding box.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"626px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"77.5%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAQABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAMEBf/EABYBAQEBAAAAAAAAAAAAAAAAAAIAAf/aAAwDAQACEAMQAAAB3lNjQpElv//EABoQAAIDAQEAAAAAAAAAAAAAAAECABESAwT/2gAIAQEAAQUCuOTlLzU9AbPAsef/xAAXEQADAQAAAAAAAAAAAAAAAAAAAiEB/9oACAEDAQE/AXii3D//xAAXEQADAQAAAAAAAAAAAAAAAAAAATEC/9oACAECAQE/Ac0dP//EABoQAAEFAQAAAAAAAAAAAAAAAAABEBEiMRL/2gAIAQEABj8CKaJOsnMlj//EABoQAQEAAwEBAAAAAAAAAAAAAAEAESExQXH/2gAIAQEAAT8hQezW7z41uQxl5dw3YMjjd//aAAwDAQACAAMAAAAQBM//xAAWEQEBAQAAAAAAAAAAAAAAAAABABH/2gAIAQMBAT8QWjLFv//EABYRAQEBAAAAAAAAAAAAAAAAAAEAEf/aAAgBAgEBPxA6SOIv/8QAHRAAAgIDAAMAAAAAAAAAAAAAAREAITFRYUGBsf/aAAgBAQABPxBMwvEVwEgBrd55C+faPqJmMaKlBIzedpdM84CCIpCf/9k=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"iou-concept\",\n    \"title\": \"iou-concept\",\n    \"src\": \"/static/d6a66a69ad780b5b37a3e447d97b704d/fb86e/iou-concept.jpg\",\n    \"srcSet\": [\"/static/d6a66a69ad780b5b37a3e447d97b704d/46946/iou-concept.jpg 240w\", \"/static/d6a66a69ad780b5b37a3e447d97b704d/55489/iou-concept.jpg 480w\", \"/static/d6a66a69ad780b5b37a3e447d97b704d/fb86e/iou-concept.jpg 626w\"],\n    \"sizes\": \"(max-width: 626px) 100vw, 626px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \")), mdx(\"p\", null, \"By Comparing mAP of our model(0.319) to it of SSD MobileNet v2 320x320(0.202) provided by TensorFlow 2 Detection Model Zoo, the model seems to be trained well.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319\\nAverage Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.670\\nAverage Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.240\\n\")), mdx(\"p\", null, \"The Average Recall(AR) was split by the max number of detection per image (1, 10, 100). The recall is 27.4% when there is only one person with helmet per image. However, when there are 100 people, the recall goes up to 46.8%.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.274\\nAverage Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.440\\nAverage Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\\n\")), mdx(\"h4\", null, \"Inference test images\"), mdx(\"p\", null, \"To check if the model inferences appropriately, I customized \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/hugozanini/object-detection/blob/master/inferenceutils.py\"\n  }, \"inferenceutil file\"), \" from the blog post. Since file extension of our image data is png, I needed to convert 4 layers(RGB and transparency) into 3 layers(RGB).\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"def load_image_into_numpy_array(path):\\n  img_data = tf.io.gfile.GFile(path, 'rb').read()\\n  image = Image.open(BytesIO(img_data))\\n  (im_width, im_height) = image.size\\n    image_np_array = np.array(image.getdata())\\n  return image_np_array[...,:3].reshape(\\n      (im_height, im_width, 3)).astype(np.uint8)\\n\")), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"400px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"96.66666666666666%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAATABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAEFA//EABYBAQEBAAAAAAAAAAAAAAAAAAABAv/aAAwDAQACEAMQAAAB6TP6JoMokpNBl//EAB0QAQABAwUAAAAAAAAAAAAAAAECAAMhERITI0L/2gAIAQEAAQUC3dVuUWmcR551LDq0gXfCZ//EABcRAAMBAAAAAAAAAAAAAAAAAAABEhD/2gAIAQMBAT8Bklb/AP/EABcRAQADAAAAAAAAAAAAAAAAAAEAEBP/2gAIAQIBAT8BWaN//8QAHhAAAQMFAQEAAAAAAAAAAAAAAQARMQIQEiFRcYH/2gAIAQEABj8CyZ0Tl8KlCmkaHEWgxbXEPbf/xAAdEAEAAgICAwAAAAAAAAAAAAABABEhMUFhgZGh/9oACAEBAAE/IRg41tMQJo+CURg0R7z3Ea1JKtsNgxl8gt3MBaf/2gAMAwEAAgADAAAAECAYwv/EABYRAQEBAAAAAAAAAAAAAAAAAAEQEf/aAAgBAwEBPxAGQZ//xAAWEQEBAQAAAAAAAAAAAAAAAAABIFH/2gAIAQIBAT8QdRyH/8QAHhABAAICAgMBAAAAAAAAAAAAAQARITFBUWFx8MH/2gAIAQEAAT8QeLQR1W18R4oByPn7xHWYdFwRe4Uxe5fmo4Asl7473Fhzc0O4fcpo5zRvMUxyza9TROWf/9k=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"test-inference-1\",\n    \"title\": \"test-inference-1\",\n    \"src\": \"/static/91ef3717a62490cd18db3c044673d82e/27ec1/test_21.jpg\",\n    \"srcSet\": [\"/static/91ef3717a62490cd18db3c044673d82e/46946/test_21.jpg 240w\", \"/static/91ef3717a62490cd18db3c044673d82e/27ec1/test_21.jpg 400w\"],\n    \"sizes\": \"(max-width: 400px) 100vw, 400px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \")), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"400px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"75%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAEDBP/EABYBAQEBAAAAAAAAAAAAAAAAAAIAAf/aAAwDAQACEAMQAAAB0O04s2GL/8QAGxAAAgIDAQAAAAAAAAAAAAAAAAIEEgEDERP/2gAIAQEAAQUC9FEzZSQ1dUTtT//EABYRAAMAAAAAAAAAAAAAAAAAAAEQEf/aAAgBAwEBPwGBf//EABYRAQEBAAAAAAAAAAAAAAAAAAAhAf/aAAgBAgEBPwG6r//EABkQAAIDAQAAAAAAAAAAAAAAAAEQABEhMf/aAAgBAQAGPwLkvQybwr//xAAbEAACAgMBAAAAAAAAAAAAAAABEQAhMUFRgf/aAAgBAQABPyEuIPuYLYkoTe5dNWNQmT4ByFcn/9oADAMBAAIAAwAAABCM7//EABYRAQEBAAAAAAAAAAAAAAAAACEAAf/aAAgBAwEBPxBzIv/EABYRAQEBAAAAAAAAAAAAAAAAAAEQIf/aAAgBAgEBPxBdFj//xAAcEAEBAAICAwAAAAAAAAAAAAABEQAhMYFRYXH/2gAIAQEAAT8QDChaCJPGWqYwp4cgtP4Ya3SRyHex9YhQge1e8Cahz//Z')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"test-inference-2\",\n    \"title\": \"test-inference-2\",\n    \"src\": \"/static/0366a8e2380797ef0b0030a83d4543dd/27ec1/test_22.jpg\",\n    \"srcSet\": [\"/static/0366a8e2380797ef0b0030a83d4543dd/46946/test_22.jpg 240w\", \"/static/0366a8e2380797ef0b0030a83d4543dd/27ec1/test_22.jpg 400w\"],\n    \"sizes\": \"(max-width: 400px) 100vw, 400px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \")), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"400px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"78.33333333333333%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAQABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAQFAQP/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAAB4ZVURQsC/wD/xAAZEAADAQEBAAAAAAAAAAAAAAABAgMSESL/2gAIAQEAAQUCFeHfpptp56Np6dVHP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABwQAAMAAQUAAAAAAAAAAAAAAAABEQIhMUFRYf/aAAgBAQAGPwKbEHoLJcCyinp0f//EABoQAQADAQEBAAAAAAAAAAAAAAEAESExUWH/2gAIAQEAAT8h0xrkqBdWo77/AGHTbhgULpBTXW0T/9oADAMBAAIAAwAAABBIz//EABURAQEAAAAAAAAAAAAAAAAAAAEA/9oACAEDAQE/EEgv/8QAFhEBAQEAAAAAAAAAAAAAAAAAAAER/9oACAECAQE/EG1//8QAGxABAQADAQEBAAAAAAAAAAAAAREAITFxQVH/2gAIAQEAAT8QAoTAd9uJ006fiu7isBSGHsxuCozWy175m+yqa/BXjiUyAlAsz//Z')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"test-inference-3\",\n    \"title\": \"test-inference-3\",\n    \"src\": \"/static/026628cdafeeacc2d35cbb6454e6d06b/27ec1/test_16.jpg\",\n    \"srcSet\": [\"/static/026628cdafeeacc2d35cbb6454e6d06b/46946/test_16.jpg 240w\", \"/static/026628cdafeeacc2d35cbb6454e6d06b/27ec1/test_16.jpg 400w\"],\n    \"sizes\": \"(max-width: 400px) 100vw, 400px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \")), mdx(\"h2\", null, \"Deploying real-time inference application\"), mdx(\"h4\", null, \"Converting the model\"), mdx(\"p\", null, \"Thanks to \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/tensorflow/tfjs/tree/master/tfjs-converter\"\n  }, \"tensorflow.js converter\"), \",\\nwe can convert trained model into JSON format(+ weights file in binary format).\\nI converted it in local rather than on colab environment.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"$ pip install tensorflowjs[wizard]\\n$ tensorflowjs_converter \\\\\\n    --input_format=tf_saved_model \\\\\\n    --output_format=tfjs_graph_model \\\\\\n    --signature_name=serving_default \\\\\\n    --saved_model_tags=serve \\\\\\n    saved_model \\\\\\n    web_model\\n\")), mdx(\"p\", null, \"While they used Tensorflow wizard to convert the model in the blog post, I used CLI command.\\nThe converted model from Tensorflow wizard caused error when I tried to inference on Application.\"), mdx(\"p\", null, \"After successful conversion, you can the model.json file which has meta data of trained model.\\nYou might have folder structured like below.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"web_model\\n|_ model.json\\n|_ group1-shard1of5.bin\\n|_ group1-shard2of5.bin\\n|_ group1-shard3of5.bin\\n|_ group1-shard4of5.bin\\n|_ group1-shard5of5.bin\\n\")), mdx(\"h4\", null, \"Time to write some code\"), mdx(\"p\", null, \"Now we are ready to develop a react-native application which can inference the image on device.\\n(Entire code for the application is available in my \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/Pigrabbit/rn-object-detection\"\n  }, \"github repository\"), \")\"), mdx(\"p\", null, \"First of all, \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://reactnative.dev/blog/2017/03/13/introducing-create-react-native-app\"\n  }, \"create React Native application\"), \",\\nand place converted model in the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"assets/\"), \" directory.\"), mdx(\"p\", null, \"We need few tensorflow packages in order to load model and inference.\\nBe careful, there are lots of dependencies to configure.\\nFollow guide on \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.npmjs.com/package/@tensorflow/tfjs-react-native\"\n  }, \"tfjs-react-native\"), \".\\nAfter configuration, open your editor and let's write some code.\"), mdx(\"p\", null, \"Using IIFE(Immediately Invoked Function Expression), we are going to load the model to application as following code.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-tsx\"\n  }, \"// App.tsx\\nimport { loadGraphModel } from \\\"@tensorflow/tfjs\\\";\\nimport {\\n  bundleResourceIO,\\n  fetch,\\n  decodeJpeg,\\n} from \\\"@tensorflow/tfjs-react-native\\\";\\n\\nlet MODEL: unknown = null;\\n\\nconst modelJSON = require(\\\"./assets/web_model/model.json\\\");\\nconst modelWeights = [\\n  require(\\\"./assets/web_model/group1-shard1of5.bin\\\"),\\n  require(\\\"./assets/web_model/group1-shard2of5.bin\\\"),\\n  require(\\\"./assets/web_model/group1-shard3of5.bin\\\"),\\n  require(\\\"./assets/web_model/group1-shard4of5.bin\\\"),\\n  require(\\\"./assets/web_model/group1-shard5of5.bin\\\"),\\n];\\n\\n(async () => {\\n  try {\\n    await tf.ready();\\n    MODEL = await loadGraphModel(bundleResourceIO(modelJSON, modelWeights));\\n  } catch (error) {\\n    console.error(error);\\n  }\\n})();\\n\")), mdx(\"p\", null, \"I used \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/react-native-camera/react-native-camera\"\n  }, \"react-native-camera\"), \" library to take photo to inference.\\n\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"App\"), \" component renders camera view before taking a photo.\\nAfter taking a photo, it replace camera view with image view.\\nIf objects detected in the image after inference, it will render detection box over the image.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-tsx\"\n  }, \"import { RNCamera } from 'react-native-camera';\\n\\nconst App = () => {\\n    // ...\\n    return (\\n        <>\\n      <StatusBar barStyle=\\\"dark-content\\\" />\\n      <SafeAreaView style={styles.container}>\\n                <Header />\\n                <View>\\n                    {imageUri === null && (\\n            <View style={styles.cameraContainer}>\\n              <RNCamera\\n                ref={cameraRef}\\n                ratio={'1:1'}\\n                style={{width: imageWidth, height: imageWidth}}\\n                onCameraReady={() => setIsReadyToCapture(true)}\\n                                ...\\n              />\\n            </View>\\n          )}\\n                    {imageUri !== null && (\\n            <View style={styles.imageContainer}>\\n              {detectionObjects &&\\n                detectionObjects.map((obj, idx) => (\\n                  <DetectedBox\\n                    key={idx}\\n                    x={obj.bbox[0]}\\n                    y={obj.bbox[1]}\\n                    width={obj.bbox[2]}\\n                    height={obj.bbox[3]}\\n                    score={obj.score}\\n                  />\\n                ))}\\n              <Image source={{uri: imageUri}} style={styles.image} />\\n            </View>\\n          )}\\n                </View>\\n                <View style={styles.controlPanel}>\\n          <View style={styles.buttonList}>\\n                        <Button title={'refresh'} ... />\\n                        <Button title={'shoot'} ... />\\n                        <Button title={'infer'} ... />\\n                    </View>\\n        </View>\\n            </SafeAreaView>\\n    </>\\n  )\\n}\\n\")), mdx(\"p\", null, \"The component manages prediction result and detection objects as state.\\nPrediction result is data of boxes, scored and classes from the output of inference.\\nDetection objects are processed object in order to render bounding box and score on the image.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-tsx\"\n  }, \"const App = () => {\\n    const [predictedResult, setPredictedResult] = useState<{\\n    boxes: unknown;\\n    scores: unknown;\\n    classes: unknown;\\n  }>({boxes: null, scores: null, classes: null});\\n  const [detectionObjects, setDetectionObjects] = useState<\\n    {\\n      bbox: number[];\\n      class: number;\\n      label: string;\\n      score: string;\\n    }[]\\n  >([]);\\n\\n    // ...\\n\\n    return (\\n        // ...\\n    )\\n}\\n\")), mdx(\"p\", null, \"Last but not least, here is how we do inference on react native application.\\nWhen user press \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"infer\"), \" button, it converts taken photo into tensor(using \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"fetch\"), \", \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"decodeJpeg\"), \" from tfjs-react-native).\\nThen this tensor passed into \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"inference()\"), \" function which executes inferencing.\\nThe output of inference is stored in component as state(\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"predictionResult\"), \").\\nWhen the prediction result changed, it generates detection objects which will be used in rendering bounding box.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-tsx\"\n  }, \"const imageWidth = Dimensions.get(\\\"window\\\").width;\\nconst threshold = 0.5;\\n\\nconst App = () => {\\n  // ...\\n\\n  const inference = useCallback(async (imageTensor: tf.Tensor3D) => {\\n    if (!imageTensor || isInferencing) return;\\n    try {\\n      const predictions: tf.Tensor<tf.Rank>[] = await MODEL.executeAsync(\\n        imageTensor.transpose([0, 1, 2]).expandDims()\\n      );\\n      const boxes = predictions[5].arraySync();\\n      const scores = predictions[4].arraySync();\\n      const classes = predictions[2].dataSync();\\n\\n      setPredictedResult({ boxes, scores, classes });\\n    } catch (error) {\\n      console.error(error);\\n    }\\n  }, []);\\n\\n  useEffect(() => {\\n    if (!predictedResult || !predictedResult.scores) return;\\n\\n    const { boxes, scores, classes } = predictedResult;\\n    const currentDetectionObjects = [];\\n\\n    scores[0].forEach((score, idx) => {\\n      if (score > threshold) {\\n        const bbox = [];\\n        const minY = boxes[0][idx][0] * imageWidth;\\n        const minX = boxes[0][idx][1] * imageWidth;\\n        const maxY = boxes[0][idx][2] * imageWidth;\\n        const maxX = boxes[0][idx][3] * imageWidth;\\n        bbox[0] = minX;\\n        bbox[1] = minY;\\n        bbox[2] = maxX - minX;\\n        bbox[3] = maxY - minY;\\n\\n        currentDetectionObjects.push({\\n          class: classes[idx],\\n          label: classesDir[classes[idx]].name,\\n          score: score.toFixed(4),\\n          bbox: bbox,\\n        });\\n      }\\n    });\\n\\n    setDetectionObjects(currentDetectionObjects);\\n  }, [predictedResult]);\\n\\n  return (\\n    // ...\\n    <Button\\n      title={\\\"infer\\\"}\\n      onPress={async () => {\\n        const response = await fetch(imageUri, {}, { isBinary: true });\\n        const imageDataArrayBuffer = await response.arrayBuffer();\\n        const imageData = new Uint8Array(imageDataArrayBuffer);\\n        const imageTensor = decodeJpeg(imageData);\\n        await inference(imageTensor);\\n      }}\\n    />\\n  );\\n};\\n\")), mdx(\"p\", null, \"It's all set, let's build the application and launch it.\\nhow some images and let it work!\"), mdx(\"p\", null, \"After numerous steps, we could eventually check if person wearing a bike helmet with react native application!\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"425px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"207.91666666666666%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAqABQDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAUBAgQD/8QAFgEBAQEAAAAAAAAAAAAAAAAAAAIB/9oADAMBAAIQAxAAAAFnODgl4JguN4aiHJOaSvGq0Eh//8QAHhAAAgICAgMAAAAAAAAAAAAAAQIAAxIhBBATMTL/2gAIAQEAAQUCZ95bDCFs5axQpYAE92CUlAvjWGtDDx6mPbfKE5T/xAAXEQADAQAAAAAAAAAAAAAAAAAAEBES/9oACAEDAQE/AaaUf//EABgRAAIDAAAAAAAAAAAAAAAAAAEQAAIR/9oACAECAQE/ARMRqH//xAAgEAACAQQBBQAAAAAAAAAAAAAAAQIREiExQRAgIjJR/9oACAEBAAY/AsXnuzkt+CcY72Zl05Y7vF10aMxKuC7/AP/EABwQAQADAAMBAQAAAAAAAAAAAAEAESExQWFREP/aAAgBAQABPyHEqj8qiVUbdh1BFa9ZapnRHmZWPhFAPPDteS/SaYot198IGvTonjhtAy6MytuVDJlVEaX8/9oADAMBAAIAAwAAABC3IAMDD//EABgRAAIDAAAAAAAAAAAAAAAAAAABESBR/9oACAEDAQE/EHggNUf/xAAZEQACAwEAAAAAAAAAAAAAAAAAARARITH/2gAIAQIBAT8Qs+4WFgw7n//EACAQAQACAgICAwEAAAAAAAAAAAEAESExQVFhgRBxoeH/2gAIAQEAAT8QNBKbT7FwFUGwL+ILS/QWziJVuHwSWUgsmDTWa7s9wy9FQqPZ5qUVsDzjP5DMTtJSTk1jf9jwUS2oYqB6KDhQibxUds+1uAOz7gBU2+YAURKUiGyGRRThfj//2Q==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"in-app-inference-1\",\n    \"title\": \"in-app-inference-1\",\n    \"src\": \"/static/6cdfa5ee6c697f4dd3b591c0c8ece2db/1cf16/app-test-small-1.jpg\",\n    \"srcSet\": [\"/static/6cdfa5ee6c697f4dd3b591c0c8ece2db/46946/app-test-small-1.jpg 240w\", \"/static/6cdfa5ee6c697f4dd3b591c0c8ece2db/1cf16/app-test-small-1.jpg 425w\"],\n    \"sizes\": \"(max-width: 425px) 100vw, 425px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \")), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"425px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"206.66666666666663%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAApABQDASIAAhEBAxEB/8QAGgAAAgMBAQAAAAAAAAAAAAAAAAQCBQYBA//EABUBAQEAAAAAAAAAAAAAAAAAAAAC/9oADAMBAAIQAxAAAAG18kYJsCvIpbmnLnLmpFSIrjR0P//EAB0QAAICAgMBAAAAAAAAAAAAAAECABEDEhMUMiD/2gAIAQEAAQUCLAxsirO3iEqyqgilig8zNai61W9Emq/DeU9T/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAERIP/aAAgBAwEBPwEjJj//xAAWEQADAAAAAAAAAAAAAAAAAAAAESD/2gAIAQIBAT8BHP8A/8QAHxAAAgEDBQEAAAAAAAAAAAAAAAECETEyECAhYZFC/9oACAEBAAY/Aq1kjmb9PpiTlfs50jXIdIvssYoxXhZbHr//xAAfEAACAgEEAwAAAAAAAAAAAAABEQAhQRAxcYEgUWH/2gAIAQEAAT8hIJ2JsmgCJ/RQXyyGMYWfG8OIPuCka2YA8QAooIhcjFTeWe1pYAFK4iHlv//aAAwDAQACAAMAAAAQ9PqwQw//xAAYEQEAAwEAAAAAAAAAAAAAAAABEBEhAP/aAAgBAwEBPxC92Jui9s//xAAcEQABAwUAAAAAAAAAAAAAAAAAAREgITFRodH/2gAIAQIBAT8QW1BqZ1weH//EACAQAQEAAQMEAwAAAAAAAAAAAAERACExUUFhcZEQocH/2gAIAQEAAT8QddiAhiS0Ok31gLTOq7jOzvUCIyGbNiWD0DKNhzhtSLaIBfzBNDD5Ye8gSQKp0TONfDcAEIrUjVwQDGwCZBsLzMAFQBd9MACAB2z6mbXh+P/Z')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"in-app-inference-2\",\n    \"title\": \"in-app-inference-2\",\n    \"src\": \"/static/3a114cd7059da0d8fed7263593e2c52d/1cf16/app-test-small-2.jpg\",\n    \"srcSet\": [\"/static/3a114cd7059da0d8fed7263593e2c52d/46946/app-test-small-2.jpg 240w\", \"/static/3a114cd7059da0d8fed7263593e2c52d/1cf16/app-test-small-2.jpg 425w\"],\n    \"sizes\": \"(max-width: 425px) 100vw, 425px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \")), mdx(\"p\", null, \"Entire code for the application is available in my \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/Pigrabbit/rn-object-detection\"\n  }, \"github repository\")), mdx(\"h2\", null, \"Conclusion\"), mdx(\"p\", null, \"You might be disappointed with recall and precision of this model.\\nHowever, what's interesting here is that we achieved client side inferencing even with cross-platfrom mobile application.\\nIt means our model is small and fast enough to run on mobile device.\\nQuite awesome, isn't it?\"), mdx(\"p\", null, \"I have done couple of deep learning projects but it was the first time I deployed the model.\\nPersonally, I think deep learning models get more value when they are used in various products(web or mobile application).\"), mdx(\"p\", null, \"All above steps were only quick prototyping, so hyper parameter tuning and other experiments are lefts to make this model (and application) better.\"), mdx(\"h2\", null, \"Reference\"), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://blog.tensorflow.org/2021/01/custom-object-detection-in-browser.html\"\n  }, \"Custom object detection in the browser using TensorFlow.js\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\"\n  }, \"TensorFlow js Model Zoo\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52\"\n  }, \"Breaking down Mean Average Precision (mAP)\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://blog.tensorflow.org/2020/02/tensorflowjs-for-react-native-is-here.html\"\n  }, \"TensorFlow.js for React Native is here!\")));\n}\n;\nMDXContent.isMDXComponent = true;","excerpt":"How I deployed 'Helmet Detection' model with React Native application The problem I wanted to solve was to check whether person wearing…","timeToRead":5,"banner":{"childImageSharp":{"resize":{"src":"/static/df7be516a4f50a9372e34836e7c91dc1/a6c62/tensorflow-js.jpg"}}}}},"pageContext":{"slug":"/deploying-deep-learning-model-with-react-native","formatString":"DD.MM.YYYY"}},"staticQueryHashes":["3090400250","3090400250","318001574"]}